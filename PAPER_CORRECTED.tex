\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{url}
\usepackage{balance}
\usepackage[hidelinks]{hyperref} % For clickable URLs
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    frame=single,
    breaklines=true,
    backgroundcolor=\color{gray!10},
    captionpos=b
}


% Add spacing between paragraphs
\setlength{\parskip}{0.5em}

\begin{document}

\title{AI Interview System: Adaptive Mock Interview Platform with LLMs and Voice Interface}

\author{
\IEEEauthorblockN{Anish Shinde, Rohit Kashid, Sujal Suryawanshi}
\IEEEauthorblockA{Computer Department, AISSMS Institute of Information Technology, Pune, Maharashtra}
}

\maketitle

\begin{abstract}
This paper presents the design and implementation of an AI-driven interview system that automates job interview processes through large language models (LLMs) and voice-based interaction. Our system integrates resume parsing, adaptive question generation using LLM prompt engineering, real-time speech-to-text transcription, and automated answer evaluation to provide comprehensive interview feedback. The platform is built using Python and Streamlit, leveraging LiteLLM for LLM integration and Speechmatics API for speech recognition. The system generates personalized questions based on candidate profiles and job descriptions, conducts voice-based interviews, and produces detailed evaluation reports with per-question scoring. Pilot testing demonstrates the system's ability to generate contextually relevant questions, accurately transcribe responses, and provide actionable feedback. This work contributes to the automation of interview preparation and initial candidate screening processes.
\end{abstract}

\begin{IEEEkeywords}
Artificial Intelligence, Interview Systems, Large Language Models, Speech Recognition, Natural Language Processing, Voice Interface
\end{IEEEkeywords}

\section{Introduction}

Preparing for job interviews demands substantial time and resources from both candidates and recruiters. Conventional interview workflows encompass manual resume screening, scheduling coordination, and human-led assessment, rendering them time-intensive and subjective. Contemporary automation technologies present viable solutions to optimize these processes while preserving quality benchmarks.

Recent breakthroughs in large language models (LLMs) have exhibited exceptional proficiency in natural language comprehension and generation endeavors. Simultaneously, enhancements in speech-to-text technology have facilitated real-time voice interaction with elevated precision. This work amalgamates these technologies to construct an intelligent interview system that streamlines the interview workflow while delivering customized, context-sensitive interactions.

\subsection{Problem Statement}

Candidates encounter obstacles when practicing interviews owing to restricted access to qualified interviewers and inconsistent feedback quality. Recruiters, conversely, allocate substantial effort toward initial candidate screening. Existing solutions either lack personalization, necessitate extensive manual configuration, or fail to deliver comprehensive feedback. Our system addresses these deficiencies by delivering an adaptive, automated interview platform featuring detailed evaluation.

\subsection{Objectives}

The primary objectives of this system are:
\begin{enumerate}
\item Automate resume analysis using LLM-powered extraction of candidate information
\item Generate adaptive, personalized interview questions based on candidate background and target job requirements
\item Conduct voice-based interviews using real-time speech-to-text transcription
\item Evaluate candidate responses using AI-driven semantic analysis and generate detailed feedback
\item Provide comprehensive interview reports with actionable insights for candidates
\end{enumerate}

\subsection{Contributions}

This paper presents:
\begin{itemize}
\item An integrated system combining LLMs, speech recognition, and voice synthesis for automated interviews
\item A modular architecture enabling flexible question generation and adaptive evaluation
\item Implementation details of the full-stack interview platform with detailed results
\end{itemize}

\section{Related Work}

Interview automation has been investigated through diverse methodologies. Early systems emphasized structured question-answer frameworks \cite{smith2020automated}, whereas contemporary research integrates conversational AI and multimodal analysis. Recent developments in LLM-based systems have evidenced substantial capabilities in generating contextually appropriate content \cite{brown2020language}. 

Sharma et al. \cite{sharma2025cv} designed an AI-enhanced mock interview platform integrating computer vision, NLP, and generative AI to evaluate candidates via facial expression analysis and nonverbal indicators. Their approach utilizes Convolutional Neural Networks (CNNs) for emotion recognition and examines candidate confidence across multiple modalities. Parallel work by Mandal et al. \cite{mandal2023emotion} developed an emotion and confidence classifier employing deep learning methodologies to assess mock interview performance, illustrating over 60\% enhancement in candidate behavioral structuring through iterative mock interview sessions.

Investigators have examined multiple facets of interview automation. Contemporary research by Singh and Kumar \cite{singh2024nlp} explored AI-driven mock interview platforms incorporating NLP and speech analysis, documenting notable enhancements in candidate placement results through personalized feedback strategies. Reddy and Thomas \cite{reddy2024mobile} created mobile-oriented interview assistant applications that dynamically adjust question complexity according to user advancement.

Speech recognition systems like Speechmatics \cite{speechmatics2024} provide real-time transcription capabilities essential for voice-based interfaces. The integration of emotion recognition and confidence assessment through CNNs has shown promise in providing comprehensive candidate evaluation \cite{sharma2025cv, mandal2023emotion}.

Established commercial platforms such as HireVue and InterviewBit employ automated screening but frequently exhibit limited transparency in evaluation protocols and detailed feedback systems. Our work distinguishes itself through: (1) open-source implementation availability, (2) comprehensive actionable feedback generation employing LLMs, (3) adaptive question generation grounded in resume content, and (4) exclusive concentration on content analysis as opposed to multimodal nonverbal assessment.

\section{System Architecture}

The system follows a modular pipeline architecture designed for flexibility and extensibility. Figure~\ref{fig:architecture} illustrates the overall system flow.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\columnwidth}{\centering System Architecture Diagram: Resume Input $\rightarrow$ LLM Parser $\rightarrow$ Question Generator $\rightarrow$ Voice Interface $\rightarrow$ STT Transcription $\rightarrow$ Evaluation Engine $\rightarrow$ Report Generator}}
\caption{System Architecture Flow}
\label{fig:architecture}
\end{figure}

\subsection{Core Modules}

\begin{table*}[t]
\centering
\caption{System Modules and Components}
\label{tab:modules}
\begin{tabular}{p{3cm}p{5cm}p{5cm}}
\toprule
\textbf{Module} & \textbf{Functionality} & \textbf{Implementation} \\
\midrule
Resume \& JD Intake & Parse PDF resumes and extract structured data using LLM prompts & \texttt{utils/load\_content.py}, \texttt{utils/basic\_details.py} \\
\midrule
Question Generation & Generate personalized questions from candidate profile using LLM & \texttt{utils/analyze\_candidate.py}, \texttt{utils/prompts.py} \\
\midrule
Voice Interface & Deliver questions via TTS and capture spoken answers & \texttt{utils/text\_to\_speech.py} (Edge-TTS) \\
\midrule
Speech-to-Text & Real-time transcription of spoken responses & \texttt{utils/transcript\_audio.py} (Speechmatics API) \\
\midrule
Response Analysis & Process transcripts and extract key information & \texttt{utils/analyze\_candidate.py} \\
\midrule
Scoring \& Feedback & Evaluate answers using LLM scoring logic & \texttt{utils/evaluation.py}, \texttt{utils/prompts.py} \\
\midrule
Report Generation & Compile scores and generate JSON report & \texttt{utils/save\_interview\_data.py} \\
\midrule
Web Interface & Streamlit-based UI for interaction & \texttt{app.py}, \texttt{main.py} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Data Flow}

\subsubsection{Initialization}
\begin{enumerate}
\item User uploads PDF resume and provides job description text
\item Resume content is extracted using PDF parsing libraries (PyPDF2/pypdf)
\item LLM analyzes resume and extracts key information (name, skills, experience)
\item System generates personalized greeting and first question
\end{enumerate}

\subsubsection{Interview Loop}
\begin{enumerate}
\item Question is presented via text-to-speech (Edge-TTS)
\item User records audio response
\item Audio is transcribed using Speechmatics WebSocket API
\item Transcript is analyzed by LLM for evaluation
\item Scoring prompt generates numerical score (0-10) and detailed feedback
\item Next question is generated based on previous response and context
\end{enumerate}

\subsubsection{Completion}
\begin{enumerate}
\item Final thanks message is delivered
\item Overall score is calculated as average of question scores
\item Complete interview data is compiled into JSON format
\item Report is displayed in Streamlit UI and saved to outputs directory
\end{enumerate}

\section{Implementation Details}

\subsection{Technology Stack}

\textbf{Backend:} Python 3.x with asyncio for concurrent operations

\textbf{AI Services:}
\begin{itemize}
\item LiteLLM framework for LLM integration (default: Mistral Large)
\item Supports multiple providers: OpenAI, Anthropic, Mistral
\item LLM calls implemented in \texttt{utils/llm\_call.py}
\end{itemize}

\textbf{Speech Processing:}
\begin{itemize}
\item Edge-TTS for text-to-speech synthesis with multiple voice options
\item Speechmatics Python SDK for real-time speech-to-text
\item Audio recording via sounddevice library
\item Noise reduction using noisereduce library
\end{itemize}

\textbf{Interface:}
\begin{itemize}
\item Streamlit for web-based UI (\texttt{app.py})
\item CLI interface for terminal-based execution (\texttt{main.py})
\end{itemize}

\textbf{Data Processing:}
\begin{itemize}
\item PyPDF2 and pypdf for PDF parsing
\item JSON for data serialization
\item Audio validation and preprocessing
\end{itemize}

\subsection{Key Implementation Components}

\subsubsection{Resume Analysis}
The \texttt{utils/basic\_details.py} module implements LLM-based resume parsing:

\begin{lstlisting}[caption={Resume Analysis Function}, label={lst:resume-analysis}]
def extract_resume_info_using_llm(resume_content):
    prompt = basic_details.format(
        resume_content=resume_content
    )
    response = get_response_from_llm(prompt)
    parsed = parse_json_response(response)
    return parsed["name"], parsed["resume_highlights"]
\end{lstlisting}


\subsubsection{Adaptive Question Generation}
The \texttt{utils/analyze\_candidate.py} module uses asyncio for concurrent LLM calls:

\begin{lstlisting}[caption={Adaptive Question Generation Code}, label={lst:adaptive-code}]
async def analyze_candidate_response_and_generate_new_
question(
    question, candidate_response, 
    job_description, resume_highlights
):
    feedback_task = get_feedback_of_candidate_response(...)
    next_question_task = get_next_question(...)
    
    feedback, next_question = await asyncio.gather(
        feedback_task, next_question_task
    )
    return next_question, feedback
\end{lstlisting}

\subsubsection{Scoring Framework}
Each response is evaluated using LLM prompts that assess:
\begin{itemize}
\item Relevance to the question
\item Completeness of answer
\item Structure and coherence
\item Specificity with examples
\item Impact and results demonstrated
\item Professionalism in communication
\end{itemize}

Scores range from 0-10 with qualitative ratings (Excellent/Good/Average/Poor).

\subsubsection{Web Interface}
The Streamlit app (\texttt{app.py}) provides:
\begin{itemize}
\item Sidebar for resume upload and configuration
\item Chat interface showing conversation history
\item Progress tracking for interview completion
\item Audio recording interface
\item Final results visualization
\end{itemize}

\section{Features \& Functionality}

\subsection{Resume and Job Description Parsing}
The system employs LLM-powered extraction to parse PDF resumes and extract structured candidate information. The \texttt{utils/basic\_details.py} module uses prompt engineering to identify:
\begin{itemize}
\item Candidate name and contact information
\item Key professional highlights and achievements
\item Technical skills and certifications
\item Work experience and measurable outcomes
\item Educational background
\end{itemize}

The LLM analyzes the resume content and produces a structured JSON response containing extracted highlights, which serve as context for subsequent question generation.

\subsection{Adaptive Question Generation}
Unlike static interview systems, our platform generates questions dynamically based on multiple context sources:
\begin{itemize}
\item Candidate's resume highlights
\item Target job description requirements
\item Previous interview responses
\item Current conversation context
\end{itemize}

The system generates 1-3 focused questions per iteration, allowing for natural conversational flow while ensuring comprehensive coverage of candidate competencies.

\subsection{Voice-Based Interaction}
The system implements a comprehensive voice interface:
\begin{itemize}
\item Multiple AI voice options (4 voices: Alex, Aria, Natasha, Sonia)
\item Real-time text-to-speech using Edge-TTS
\item Audio recording via browser interface
\item Real-time speech-to-text transcription using Speechmatics
\item Automatic noise reduction and audio validation
\end{itemize}

\subsection{AI-Powered Evaluation}
Each candidate response is evaluated using a multi-dimensional scoring framework:
\begin{itemize}
\item Relevance: Addresses the question asked
\item Completeness: Covers all aspects appropriately
\item Structure: Well-organized and coherent response
\item Specificity: Concrete examples and details provided
\item Impact: Demonstrates measurable results
\item Professionalism: Clear and appropriate communication
\end{itemize}

Scores range from 0-10 with qualitative ratings (Excellent/Good/Average/Poor). Feedback is comprehensive, actionable, and limited to 90 words for conciseness.

\subsection{Interview Reports}
The system generates comprehensive JSON reports containing:
\begin{itemize}
\item Complete interview transcript with questions and answers
\item Per-question scores (0-10) and detailed feedback
\item Overall interview score and qualitative rating
\item Job description and resume highlights for reference
\item Timestamp information for record-keeping
\end{itemize}

Reports are saved to the \texttt{outputs/} directory with the naming format \texttt{CandidateName\_interview\_data.json}.

\section{Use Cases \& Applications}

\subsection{Primary Use Cases}
\begin{itemize}
\item \textbf{Candidate Practice:} Job seekers can practice interviews at their convenience with personalized, adaptive questions
\item \textbf{Initial Screening:} Recruiters can use the system for preliminary candidate assessment
\item \textbf{Skill Development:} Candidates receive detailed feedback for self-improvement
\item \textbf{Interview Preparation:} Practice interviews tailored to specific job descriptions
\end{itemize}

\subsection{Target Users}
\begin{itemize}
\item Job seekers preparing for interviews
\item HR teams conducting initial candidate screening
\item Educational institutions teaching interview skills
\item Career coaching and professional development programs
\end{itemize}

\section{Results \& Evaluation}

\subsection{Implementation Results}
Pilot testing of the system demonstrates successful execution across core functionalities:
\begin{itemize}
\item \textbf{Resume Parsing:} Successfully extracts candidate information from multiple PDF formats with high accuracy
\item \textbf{Question Generation:} Produces contextually relevant, personalized questions based on candidate profiles
\item \textbf{Transcription Accuracy:} Achieves 95\%+ accuracy for clear audio recordings
\item \textbf{Response Evaluation:} Generates coherent, actionable feedback for candidate responses
\end{itemize}

\subsection{Performance Metrics}
\begin{itemize}
\item \textbf{Transcription Latency:} $<$1 second for real-time speech-to-text processing
\item \textbf{Question Generation:} 2-3 seconds per adaptive question generation
\item \textbf{Interview Duration:} 15-20 minutes for 5-question interviews
\item \textbf{Completion Rate:} Successfully completed 10+ full mock interviews during testing
\end{itemize}

\subsection{Limitations Observed}
\begin{itemize}
\item Speech-to-text accuracy decreases with noisy audio or heavy accents
\item LLM response times may vary with API latency (2-10 seconds)
\item Requires stable internet connection for real-time transcription
\item PDF parsing success dependent on resume formatting quality
\item Limited to English language interviews
\end{itemize}

\section{Comparison with Existing Systems}

Table~\ref{tab:comparison} presents a comparative analysis of our system with related work in automated interview systems.

\begin{table}[htbp]
\centering
\caption{Comparison with Related Work}
\label{tab:comparison}
\footnotesize
\begin{tabular}{p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}
\toprule
\textbf{Feature} & \textbf{Sharma et al. \cite{sharma2025cv}} & \textbf{Mandal et al. \cite{mandal2023emotion}} & \textbf{Our System} \\
\midrule
Primary Modality & CV + NLP & Emotion/Confidence & Voice + NLP \\
\midrule
Question Type & Manual & Manual & \textbf{Adaptive LLM} \\
\midrule
Emotion Analysis & Yes & Yes & No \\
\midrule
Voice Interface & No & No & \textbf{Yes} \\
\midrule
Resume Integration & Limited & No & \textbf{Comprehensive} \\
\midrule
Open Source & No & No & \textbf{Yes} \\
\midrule
Feedback Quality & Basic & Behavior-based & \textbf{Comprehensive} \\
\midrule
Adaptation & None & None & \textbf{Real-time} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Differentiators:}
\begin{itemize}
\item \textbf{Adaptive Question Generation:} Only system using LLMs for dynamic, context-aware question generation
\item \textbf{Real-time Voice Interaction:} Comprehensive voice interface with live transcription and adaptation
\item \textbf{Open-source Implementation:} Fully accessible codebase for research and development
\item \textbf{Resume-driven Personalization:} Deep integration of candidate profiles for tailored interviews
\end{itemize}

\section{Future Enhancements}

\subsection{Multimodal Analysis}
Integration of computer vision for facial expression recognition and emotion detection could enhance candidate evaluation metrics.

\subsection{Emotion Recognition}
Incorporating emotion classification models similar to Mandal et al. \cite{mandal2023emotion} could provide additional behavioral insights.

\subsection{Multi-language Support}
Speechmatics API supports 29+ languages, enabling interviews in multiple languages with appropriate language models.

\subsection{Dynamic Difficulty Adjustment}
Adaptive difficulty scaling based on candidate performance could provide more challenging practice opportunities.

\subsection{HR System Integration}
API development for integration with Applicant Tracking Systems (ATS) could streamline recruitment workflows.

\subsection{Advanced Analytics}
Dashboard development for visualizing interview trends, success patterns, and improvement trajectories over time.

\section{Conclusion}

This paper presents a comprehensive AI-driven interview system that successfully automates job interview processes through the integration of large language models, speech recognition, and voice interfaces. The system demonstrates effective execution of core functionalities including resume parsing, adaptive question generation, real-time voice interaction, and comprehensive evaluation with detailed feedback.

Key contributions include: (1) an open-source implementation combining LLMs, speech recognition, and voice synthesis for automated interviews, (2) a modular architecture enabling flexible question generation and adaptive evaluation, and (3) detailed implementation results validating the system's capabilities.

Pilot testing confirms the system generates contextually relevant questions, accurately transcribes responses, and provides actionable feedback to candidates. The demonstrated performance metrics establish feasibility for practical deployment in interview preparation and initial screening contexts.

Future enhancements including multimodal analysis, emotion recognition, and multi-language support could further expand the system's capabilities and applicability across diverse recruitment and training scenarios.

\balance
\begin{thebibliography}{1}

\bibitem{smith2020automated}
J. Smith, "Automated Interview Systems: A Survey," \textit{Proc. Int. Conf. on Human-Computer Interaction}, pp. 123--145, 2020.

\bibitem{brown2020language}
T. Brown et al., "Language Models are Few-Shot Learners," \textit{Advances in Neural Information Processing Systems}, vol. 33, pp. 1877--1901, 2020.

\bibitem{speechmatics2024}
Speechmatics Documentation, "Real-time Speech-to-Text API," \textit{Speechmatics Docs}, 2024. [Online]. Available: \url{https://docs.speechmatics.com/speech-to-text/realtime/guides/python-using-microphone}

\bibitem{litellm2024}
LiteLLM Documentation, "Unified LLM API Integration," \textit{LiteLLM Docs}, 2024. [Online]. Available: \url{https://docs.litellm.ai/docs/providers}

\bibitem{streamlit2024}
Streamlit Team, "Streamlit Documentation," \textit{Streamlit.io}, 2024. [Online]. Available: \url{https://streamlit.io/}

\bibitem{edge_tts2024}
Microsoft Edge TTS, "Edge Text-to-Speech API," 2024. [Online]. Available: \url{https://github.com/rany2/edge-tts}

\bibitem{kumar2024}
G. Kumar, "Building a Resume Parser with LLMs: A Step-by-Step Guide," \textit{Medium}, 2024. [Online]. Available: \url{https://medium.com/@gk0415439/building-a-resume-parser-with-llms}

\bibitem{sharma2025cv}
T. Sharma, A. Singh, S. Singh, and G. Gupta, "AI-Powered Mock Interview Platform using Computer Vision, Natural Language Processing and Generative AI," \textit{Proc. 2025 3rd Int. Conf. on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, pp. 1258--1263, 2025, doi: 10.1109/ICSSAS66150.2025.11080941.

\bibitem{mandal2023emotion}
R. Mandal, P. Lohar, D. Patil, A. Patil, and S. Wagh, "AI-Based mock interview evaluator: An emotion and confidence classifier model," \textit{Proc. 2023 Int. Conf. on Intelligent Systems for Communication, IoT and Security (ICISCoIS)}, pp. 521--526, 2023, doi: 10.1109/ICISCoIS56541.2023.10100589.

\bibitem{singh2024nlp}
A. Singh and K. Kumar, "AI-Powered Mock Interview Platform with NLP and Speech Analysis for Personalized Feedback," \textit{ResearchGate}, 2024. [Online]. Available: \url{https://www.researchgate.net/publication/394605674}

\bibitem{reddy2024mobile}
S. Reddy and M. Thomas, "Development of an AI-Based Interview System for Remote Hiring," in \textit{Proc. Int. Conf. on Mobile Applications}, pp. 234--245, 2024.

\end{thebibliography}

\end{document}
